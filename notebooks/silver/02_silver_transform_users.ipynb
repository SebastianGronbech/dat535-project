{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Yelp Users Silver Layer Transformation\n",
    "\n",
    "This notebook ingests Yelp user data from the Bronze layer, parses JSON records, validates them, deduplicates and transforms them into a Silver layer format using **PySpark RDDs** and writes in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import (IntegerType, StringType, StructField,\n",
    "                                   StructType)\n",
    "\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "# Initialize SparkSession and SparkContext\n",
    "if pyspark_available:\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"yelp_review_silver_transform\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    print(\"Spark session initialzed succesfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI available at: {sc.uiWebUrl}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tasks - Pyspark not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_safe(json_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Safely parse a JSON string and add ingestion metadata.\n",
    "\n",
    "    Args:\n",
    "        json_str (str): The JSON string to parse.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the parsed data and ingestion metadata,\n",
    "              or error information if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "\n",
    "        # Add ingestion metadata\n",
    "        data[\"_ingestion_date\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        data[\"_ingestion_timestamp\"] = time.time()\n",
    "        data[\"_source\"] = \"yelp_dataset\"\n",
    "        data[\"_status\"] = \"valid\"\n",
    "\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "\n",
    "        return {\n",
    "            \"_raw_data\": json_str,\n",
    "            \"_ingestion_timestamp\": time.time(),\n",
    "            \"_source\": \"yelp_dataset\",\n",
    "            \"_status\": \"parse_error\",\n",
    "            \"_error_msg\": str(e),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_user_valid(user: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validate a user record based on required fields and types.\n",
    "\n",
    "    Args:\n",
    "        user (dict): The user record to validate.\n",
    "    Returns:\n",
    "        bool: True if the user record is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    required_fields = [\n",
    "        \"user_id\",\n",
    "        \"name\",\n",
    "        \"review_count\",\n",
    "        \"average_stars\",\n",
    "        \"yelping_since\",\n",
    "    ]\n",
    "\n",
    "    # 1. Check required fields exist\n",
    "    if not all(field in user for field in required_fields):\n",
    "        return False\n",
    "\n",
    "    # 2. Check user_id and name are non-empty strings\n",
    "    if not isinstance(user[\"user_id\"], str) or len(user[\"user_id\"].strip()) == 0:\n",
    "        return False\n",
    "    if not isinstance(user[\"name\"], str) or len(user[\"name\"].strip()) == 0:\n",
    "        return False\n",
    "\n",
    "    # 3. Validate numeric fields (allow int, float, numeric strings)\n",
    "    try:\n",
    "        if int(user[\"review_count\"]) < 0:\n",
    "            return False\n",
    "\n",
    "        float(user[\"average_stars\"])\n",
    "\n",
    "        datetime.strptime(user[\"yelping_since\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_user_silver(user: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Transform a valid user record to the silver schema.\n",
    "\n",
    "    Args:\n",
    "        user (dict): The valid user record to transform.\n",
    "    Returns:\n",
    "        dict: The transformed user record.\n",
    "    \"\"\"\n",
    "    transformed_user = {\n",
    "        \"user_id\": user[\"user_id\"].strip(),\n",
    "        \"name\": user[\"name\"].strip(),\n",
    "        \"review_count\": int(user[\"review_count\"]),\n",
    "        \"average_stars\": float(user[\"average_stars\"]),\n",
    "        \"yelping_since\": user[\"yelping_since\"],\n",
    "        \"ingest_date\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"),\n",
    "    }\n",
    "\n",
    "    return transformed_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_user.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 /data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_user.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Load bronze data as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"file:///data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_user.json\"\n",
    "if pyspark_available:\n",
    "    users_raw_rdd = sc.textFile(raw_path)\n",
    "    users_parsed_rdd = users_raw_rdd.map(parse_json_safe)\n",
    "    print(\"Parsed record count:\", users_parsed_rdd.count())\n",
    "    print(\"Parsed sample line:\", users_parsed_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Filter parsable users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_valid_json_rdd = users_parsed_rdd.filter(lambda d: d[\"_status\"] == \"valid\")\n",
    "    users_invalid_json_rdd = users_parsed_rdd.filter(\n",
    "        lambda d: d[\"_status\"] == \"parse_error\"\n",
    "    )\n",
    "\n",
    "    total_count = users_parsed_rdd.count()\n",
    "    invalid_count = users_invalid_json_rdd.count()\n",
    "    print(\n",
    "        f\"Malformed records: {invalid_count}/{total_count} ({invalid_count/total_count*100:.2f}%)\"\n",
    "    )\n",
    "    print(f\"Valid records: {users_valid_json_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Filter valid users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_valid_rdd = users_valid_json_rdd.filter(is_user_valid)\n",
    "\n",
    "    print(f\"Valid users records: {users_valid_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Deduplicate users by `user_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_deduped_rdd = (\n",
    "        users_valid_rdd.map(lambda r: (r[\"user_id\"], r))\n",
    "        .reduceByKey(lambda a, b: a)\n",
    "        .map(lambda kv: kv[1])\n",
    "    )\n",
    "\n",
    "    print(\"After deduplication:\", users_deduped_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Apply silver transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_silver_rdd = users_deduped_rdd.map(transform_user_silver)\n",
    "    print(\"Transformed users record count:\", users_silver_rdd.count())\n",
    "    print(\"Sample transformed users record:\", users_silver_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Convert RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_silver_schema = StructType(\n",
    "        [\n",
    "            StructField(\"user_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), False),\n",
    "            StructField(\"review_count\", IntegerType(), False),\n",
    "            StructField(\"average_stars\", StringType(), False),\n",
    "            StructField(\"yelping_since\", StringType(), False),\n",
    "            StructField(\"ingest_date\", StringType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    users_silver_df = spark.createDataFrame(\n",
    "        users_silver_rdd, schema=users_silver_schema\n",
    "    )\n",
    "    users_silver_df.printSchema()\n",
    "    users_silver_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Write silver data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    users_silver_path = \"file:///data/silver/yelp/users/\"\n",
    "    users_silver_df.write.mode(\"overwrite\").partitionBy(\"ingest_date\").parquet(\n",
    "        users_silver_path\n",
    "    )\n",
    "    print(f\"Users silver data written to: {users_silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
