{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09219dcb",
   "metadata": {},
   "source": [
    "# Yelp Business Silver Layer Tranformation\n",
    "\n",
    "This notebook ingest the Yelp business data form th Bronze layer, parses JSON records, validates them, deduplicates and tranforms them int a Silver layer format using **PySpark RDDs** and wirets in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753425af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import ( StringType, StructField,\n",
    "                                   StructType)\n",
    "\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "# Initialize SparkSession and SparkContext\n",
    "if pyspark_available:\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"yelp_business_silver_transform\")\n",
    "        .master(\"spark://192.168.5.121:7077\")\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "        # .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        # .config(\"spark.dynamicAllocation.minExecutors\", \"3\")\n",
    "        # .config(\"spark.dynamicAllocation.maxExecutors\", \"16\")\n",
    "\n",
    "        # Executor settings\n",
    "        .config(\"spark.executor.cores\", \"4\")\n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "\n",
    "        # Driver memory\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "\n",
    "        # Parallelism settings\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"24\")\n",
    "        .config(\"spark.default.parallelism\", \"24\")\n",
    "        \n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    print(\"Spark session initialzed succesfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI available at: {sc.uiWebUrl}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tasks - Pyspark not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52675df7",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_safe(json_str: str) -> dict:\n",
    "    \"\"\"\n",
    "    Safely parse a JSON string and add ingestion metadata.\n",
    "\n",
    "    Args:\n",
    "        json_str (str): The JSON string to parse.\n",
    "    Returns:\n",
    "        dict: A dictionary containing the parsed data and ingestion metadata,\n",
    "              or error information if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "\n",
    "        # Add ingestion metadata\n",
    "        data[\"_ingestion_date\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "        data[\"_ingestion_timestamp\"] = time.time()\n",
    "        data[\"_source\"] = \"yelp_dataset\"\n",
    "        data[\"_status\"] = \"valid\"\n",
    "\n",
    "        return data\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "\n",
    "        return {\n",
    "            \"_raw_data\": json_str,\n",
    "            \"_ingestion_timestamp\": time.time(),\n",
    "            \"_source\": \"yelp_dataset\",\n",
    "            \"_status\": \"parse_error\",\n",
    "            \"_error_msg\": str(e),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a32051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_business_valid(business: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Validate business data based on specific criteria.\n",
    "\n",
    "    Args:\n",
    "        business (dict): The business data to validate.\n",
    "    Returns:\n",
    "        bool: True if the business data is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    required_fields = [\"business_id\", \"name\", \"categories\"]\n",
    "\n",
    "    if not all(field in business for field in required_fields):\n",
    "        return False\n",
    "\n",
    "    if not isinstance(business[\"business_id\"], str) or  len(business[\"business_id\"].strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(business[\"name\"], str) or len(business[\"name\"].strip()) == 0:\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(business[\"categories\"], str) or len(business[\"categories\"].strip()) == 0:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_business_silver(business: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Transform business data to silver schema.\n",
    "\n",
    "    Args:\n",
    "        business (dict): The business data to transform.\n",
    "    Returns:\n",
    "        dict: The transformed business data.\n",
    "    \"\"\"\n",
    "    raw_cat = business.get(\"categories\")\n",
    "    if raw_cat is None or len(raw_cat.strip()) == 0:\n",
    "        categories = [\"Unknown\"]\n",
    "    else:\n",
    "        categories = [cat.strip() for cat in raw_cat.split(\",\") if cat.strip()]\n",
    "    \n",
    "    return {\n",
    "        \"business_id\": business[\"business_id\"],\n",
    "        \"name\": business[\"name\"],\n",
    "        \"categories\": categories,\n",
    "        \"ingest_date\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83284aec",
   "metadata": {},
   "source": [
    "## Quick santity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9644ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_business.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 /data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_business.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c5a0a",
   "metadata": {},
   "source": [
    "## Load bronze data as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9308ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"file:///data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_business.json\"\n",
    "if pyspark_available:\n",
    "    business_raw_rdd = sc.textFile(raw_path)\n",
    "    business_parsed_rdd = business_raw_rdd.map(parse_json_safe)\n",
    "    print(\"Parsed record count:\", business_parsed_rdd.count())\n",
    "    print(\"Parsed sample line:\", business_parsed_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3930ea",
   "metadata": {},
   "source": [
    "## Filter parable business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_valid_json_rdd = business_parsed_rdd.filter(lambda d: d[\"_status\"] == \"valid\")\n",
    "    business_invalid_json_rdd = business_parsed_rdd.filter(\n",
    "        lambda d: d[\"_status\"] == \"parse_error\"\n",
    "    )\n",
    "\n",
    "    total_count = business_parsed_rdd.count()\n",
    "    invalid_count = business_invalid_json_rdd.count()\n",
    "    print(\n",
    "        f\"Malformed records: {invalid_count}/{total_count} ({invalid_count/total_count*100:.2f}%)\"\n",
    "    )\n",
    "    print(f\"Valid records: {business_valid_json_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78900c8",
   "metadata": {},
   "source": [
    "## Filter valid business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_valid_rdd = business_valid_json_rdd.filter(is_business_valid)\n",
    "\n",
    "    print(f\"Valid business records: {business_valid_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63079d39",
   "metadata": {},
   "source": [
    "## Deduplicate business by `business_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9356329",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_deduped_rdd = (\n",
    "        business_valid_rdd.map(lambda r: (r[\"business_id\"], r))\n",
    "        .reduceByKey(lambda a, b: a)\n",
    "        .map(lambda kv: kv[1])\n",
    "    )\n",
    "\n",
    "    print(\"After deduplication:\", business_deduped_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28ce67",
   "metadata": {},
   "source": [
    "## Apply silver transformattion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_silver_rdd = business_deduped_rdd.map(transform_business_silver)\n",
    "    print(\"Transformed business record count:\", business_silver_rdd.count())\n",
    "    print(\"Sample transformed business record:\", business_silver_rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f600e0",
   "metadata": {},
   "source": [
    "## Convert RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_silver_schema = StructType(\n",
    "        [\n",
    "            StructField(\"business_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), False),\n",
    "            StructField(\"categories\", StringType(), False),\n",
    "            StructField(\"ingest_date\", StringType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    business_silver_df = spark.createDataFrame(business_silver_rdd, schema=business_silver_schema)\n",
    "    business_silver_df.printSchema()\n",
    "    business_silver_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397232d2",
   "metadata": {},
   "source": [
    "## Write silver data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd2ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    business_silver_path = \"file:///data/silver/yelp/business/\"\n",
    "    business_silver_df.write.mode(\"overwrite\").partitionBy(\"ingest_date\").parquet(\n",
    "        business_silver_path\n",
    "    )\n",
    "    print(f\"Users silver data written to: {business_silver_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef00a3",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608208aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
