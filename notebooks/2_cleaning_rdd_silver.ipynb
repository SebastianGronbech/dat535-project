{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2188f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "try: \n",
    "    from pyspark.sql import SparkSession\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "# Initialize SparkSession and SparkContext\n",
    "if pyspark_available:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Yelp_bronze_cleaning_rdd\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    print(f\"Spark session initialzed succesfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI available at: {sc.uiWebUrl}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tasks - Pyspark not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a895156",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_path = \"file:///data/bronze/yelp/raw/2025-11-13/yelp_academic_dataset_review.json\"\n",
    "if pyspark_available:\n",
    "    bronze_rdd = sc.textFile(raw_path)\n",
    "    print(\"Raw record count:\", bronze_rdd.count())\n",
    "    print(\"Sample line:\", bronze_rdd.take(1)[0][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24371f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_safe(json_str):\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        # Add ingestion metadata\n",
    "        data['_ingestion_timestamp'] = time.time()\n",
    "        data['_source'] = 'yelp_dataset'\n",
    "        data['_status'] = 'valid'\n",
    "        return data\n",
    "    except:\n",
    "        return {\n",
    "            '_raw_data': json_str,\n",
    "            '_ingestion_timestamp': time.time(),\n",
    "            '_source': 'yelp_dataset',\n",
    "            '_status': 'parse_error'\n",
    "        }\n",
    "\n",
    "if pyspark_available:\n",
    "    parsed_rdd = bronze_rdd.map(parse_json_safe)\n",
    "\n",
    "    valid_rdd = parsed_rdd.filter(lambda d: d['_status'] == 'valid')\n",
    "    invalid_rdd = parsed_rdd.filter(lambda d: d['_status'] == 'parse_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c8c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    total = bronze_rdd.count()\n",
    "    bad_count = invalid_rdd.count()\n",
    "    print(f\"Malformed records: {bad_count}/{total} ({bad_count/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = valid_rdd.take(1)[0]\n",
    "print(type(sample))\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7db1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(parsed_rdd.take(1)[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1880b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_review(r):\n",
    "    required = [\"review_id\", \"user_id\", \"business_id\", \"stars\", \"date\", \"text\", \"useful\", \"funny\", \"cool\"]\n",
    "    if not all(k in r for k in required):\n",
    "        return False\n",
    "    if r[\"text\"] is None or len(r[\"text\"].strip()) == 0:\n",
    "        return False\n",
    "    try:\n",
    "        stars = int(r[\"stars\"])\n",
    "        if stars < 1 or stars > 5:\n",
    "            return False\n",
    "        \n",
    "        for field in [\"useful\", \"funny\", \"cool\"]:\n",
    "            if int(r[field]) < 0:\n",
    "                return False\n",
    "            \n",
    "        # Try both possible formats\n",
    "        try:\n",
    "            datetime.strptime(r[\"date\"], \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            datetime.strptime(r[\"date\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "    \n",
    "if pyspark_available:\n",
    "    valid_rdd_2 = valid_rdd.filter(valid_review)\n",
    "    print(\"Valid records:\", valid_rdd_2.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped = valid_rdd_2.map(lambda r: (r[\"review_id\"], r)) \\\n",
    "                   .reduceByKey(lambda a, b: a) \\\n",
    "                   .map(lambda kv: kv[1])\n",
    "\n",
    "print(\"After deduplication:\", deduped.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_re = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "def normalize_review(r):\n",
    "    # Lowercase and remove punctuation\n",
    "    text = r[\"text\"].lower()\n",
    "    text = punct_re.sub('', text).strip()\n",
    "    \n",
    "    # Normalize date to ISO format\n",
    "    try:\n",
    "        dt = time.datetime.strptime(r[\"date\"], \"%Y-%m-%d\")\n",
    "        r[\"date\"] = dt.strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        r[\"date\"] = None\n",
    "    \n",
    "    # Return normalized record\n",
    "    return {\n",
    "        \"review_id\": r[\"review_id\"],\n",
    "        \"user_id\": r[\"user_id\"],\n",
    "        \"business_id\": r[\"business_id\"],\n",
    "        \"stars\": float(r[\"stars\"]),\n",
    "        \"text\": text,\n",
    "        \"date\": r[\"date\"]\n",
    "    }\n",
    "\n",
    "normalized = deduped.map(normalize_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d3536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "fill_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "cleaned = normalized.map(lambda r: {**r, \"date\": r[\"date\"] or fill_date})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/data/silver/yelp/cleaned_reviews_jsonlines\"\n",
    "cleaned_json = cleaned.map(lambda r: json.dumps(r))\n",
    "\n",
    "cleaned_json.coalesce(8).saveAsTextFile(output_path)\n",
    "print(f\"âœ… Cleaned data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04511f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cleaned = cleaned.count()\n",
    "avg_length = cleaned.map(lambda r: len(r[\"text\"].split())).mean()\n",
    "star_dist = cleaned.map(lambda r: (r[\"stars\"], 1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "\n",
    "print(\"Total cleaned reviews:\", total_cleaned)\n",
    "print(\"Average review length (words):\", avg_length)\n",
    "print(\"Stars distribution:\", star_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96089ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
